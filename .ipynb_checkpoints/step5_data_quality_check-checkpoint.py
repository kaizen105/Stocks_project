# ==============================================================================
# --- Full Data Pipeline Sanity Check ---
# This script performs a comprehensive diagnostic on all key data files
# generated by the pipeline, from raw/processed inputs to the final output.
# It checks for file existence, data integrity, completeness, and structure.
# IT DOES NOT MODIFY ANY FILES.
# ==============================================================================

import pandas as pd
import os

# --- Configuration ---
# Define all the files we expect to exist and their key properties.
FILES_TO_CHECK = {
    'Price Data': {
        'path': 'data/processed/stock_prices_with_metrics.csv',
        'key_cols': ['Date', 'Ticker', 'Close', 'Volume', 'Return_Pct'],
        'has_ticker': True
    },
    'Macro Data': {
        'path': 'data/processed/macro_data_daily.csv',
        'key_cols': ['Date', 'Fed_Funds_Rate', 'CPI', 'VIX'],
        'has_ticker': False
    },
    'Final Combined Data': {
        'path': 'data/features/Stocks_dataset.csv',
        'key_cols': ['Date', 'Ticker', 'Close', 'Fed_Funds_Rate'],
        'has_ticker': True
    }
}

def run_full_sanity_check():
    """
    Orchestrates the entire sanity check process for all specified files.
    """
    print("=" * 70)
    print("RUNNING FULL DATA PIPELINE SANITY CHECK")
    print("=" * 70)
    
    master_verdict = True
    all_issues = []

    # --- Iterate through each file and run a series of checks ---
    for file_name, properties in FILES_TO_CHECK.items():
        path = properties['path']
        print(f"\n--- Checking: {file_name} ({path}) ---")
        
        # 1. File Existence
        if not os.path.exists(path):
            issue = f"File not found: {path}"
            print(f"   ❌ FAIL: {issue}")
            all_issues.append(f"[{file_name}] {issue}")
            master_verdict = False
            continue
            
        print("   ✅ File exists.")
        
        # 2. File Loading and Basic Structure
        try:
            df = pd.read_csv(path, parse_dates=['Date'])
            if df.empty:
                issue = "File is empty."
                print(f"   ❌ FAIL: {issue}")
                all_issues.append(f"[{file_name}] {issue}")
                master_verdict = False
                continue
            
            print(f"   - Shape: {df.shape[0]} rows, {df.shape[1]} columns")
            
            missing_cols = [col for col in properties['key_cols'] if col not in df.columns]
            if missing_cols:
                issue = f"Missing key columns: {', '.join(missing_cols)}"
                print(f"   ❌ FAIL: {issue}")
                all_issues.append(f"[{file_name}] {issue}")
                master_verdict = False

        except Exception as e:
            issue = f"Failed to load or parse the file. Error: {e}"
            print(f"   ❌ FAIL: {issue}")
            all_issues.append(f"[{file_name}] {issue}")
            master_verdict = False
            continue

        # 3. Data Integrity Checks
        # Check for significant missing values in key columns
        missing_check = df[properties['key_cols']].isnull().sum()
        high_missing = missing_check[missing_check > 0]
        if not high_missing.empty:
            issue = f"Found missing values in key columns: {high_missing.to_dict()}"
            print(f"   ⚠️  WARN: {issue}")
            # This is a warning, not a failure, but we'll log it.
            all_issues.append(f"[{file_name}] {issue}")

        # Check for duplicate rows
        if df.duplicated().any():
            issue = "Duplicate rows found in the dataset."
            print(f"   ❌ FAIL: {issue}")
            all_issues.append(f"[{file_name}] {issue}")
            master_verdict = False

    # --- Final Verdict ---
    print("\n" + "=" * 70)
    print("FINAL VERDICT")
    print("=" * 70)

    if not master_verdict:
        print("   ❌ SANITY CHECK FAILED.")
        print("   The following critical issues were detected:\n")
        for i, issue in enumerate(all_issues, 1):
            if "FAIL" in issue or "not found" in issue or "Missing" in issue:
                 print(f"   - {issue}")
        print("\n   Please fix the scripts that generate these files.")
    else:
        print("   ✅ SANITY CHECK PASSED.")
        print("   All key files exist and have the correct basic structure.")
        
    if any("WARN" in issue for issue in all_issues):
        print("\n   The following warnings were also noted (potential data quality issues):")
        for issue in all_issues:
            if "WARN" in issue:
                print(f"   - {issue}")


if __name__ == "__main__":
    run_full_sanity_check()

