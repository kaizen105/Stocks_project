{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e3123b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "COLUMNS AVAILABLE:\n",
      "['Date', 'Ticker', 'Reported_EPS', 'Estimated_EPS', 'Surprise', 'Surprise_Pct', 'Revenue', 'Gross_Profit', 'Net_Income', 'Operating_Income', 'EBITDA', 'Total_Assets', 'Total_Liabilities', 'Total_Equity', 'Total_Debt', 'Debt_to_Equity', 'PE_Ratio', 'Beta', 'Dividend_Yield', 'ROE', 'Profit_Margin']\n",
      "\n",
      "============================================================\n",
      "NON-NULL COUNTS:\n",
      "Date                 118\n",
      "Ticker               118\n",
      "Reported_EPS         118\n",
      "Estimated_EPS        118\n",
      "Surprise             118\n",
      "Surprise_Pct         118\n",
      "Revenue               60\n",
      "Gross_Profit          60\n",
      "Net_Income            60\n",
      "Operating_Income      60\n",
      "EBITDA                60\n",
      "Total_Assets          60\n",
      "Total_Liabilities     60\n",
      "Total_Equity          60\n",
      "Total_Debt            60\n",
      "Debt_to_Equity        60\n",
      "PE_Ratio             118\n",
      "Beta                 118\n",
      "Dividend_Yield       118\n",
      "ROE                  118\n",
      "Profit_Margin        118\n",
      "dtype: int64\n",
      "\n",
      "============================================================\n",
      "LAST 3 YEARS ONLY (2022+):\n",
      "Records from 2022+: 14\n",
      "\n",
      "Sample of recent data:\n",
      "          Date Ticker  Reported_EPS       Revenue    Net_Income\n",
      "104 2022-03-31   MSFT          2.22  4.936000e+10  1.672800e+10\n",
      "105 2022-06-30   MSFT          2.23  5.186500e+10  1.674000e+10\n",
      "106 2022-09-30   MSFT          2.35  5.012200e+10  1.755600e+10\n",
      "107 2022-12-31   MSFT          2.32  5.274700e+10  1.642500e+10\n",
      "108 2023-03-31   MSFT          2.45  5.285700e+10  1.829900e+10\n",
      "109 2023-06-30   MSFT          2.69  5.618900e+10  2.008100e+10\n",
      "110 2023-09-30   MSFT          2.99  5.651700e+10  2.229100e+10\n",
      "111 2023-12-31   MSFT          2.93  6.202000e+10  2.187000e+10\n",
      "112 2024-03-31   MSFT          2.94  6.185800e+10  2.193900e+10\n",
      "113 2024-06-30   MSFT          2.95  6.472700e+10  2.203600e+10\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data/raw/fundamentals_quarterly.csv')\n",
    "\n",
    "# Check what we actually got\n",
    "print(\"=\" * 60)\n",
    "print(\"COLUMNS AVAILABLE:\")\n",
    "print(df.columns.tolist())\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"NON-NULL COUNTS:\")\n",
    "print(df.count())\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"LAST 3 YEARS ONLY (2022+):\")\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "recent = df[df['Date'] >= '2022-01-01']\n",
    "print(f\"Records from 2022+: {len(recent)}\")\n",
    "print(\"\\nSample of recent data:\")\n",
    "print(recent[['Date', 'Ticker', 'Reported_EPS', 'Revenue', 'Net_Income']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ed7c7e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "COMPLETE 2022+ DATA CHECK\n",
      "============================================================\n",
      "\n",
      "Non-null counts for 2022+ data:\n",
      "Date                 42\n",
      "Ticker               42\n",
      "Reported_EPS         42\n",
      "Estimated_EPS        42\n",
      "Surprise             42\n",
      "Surprise_Pct         42\n",
      "Revenue              36\n",
      "Gross_Profit         36\n",
      "Net_Income           36\n",
      "Operating_Income     36\n",
      "EBITDA               36\n",
      "Total_Assets         36\n",
      "Total_Liabilities    36\n",
      "Total_Equity         36\n",
      "Total_Debt           36\n",
      "Debt_to_Equity       36\n",
      "PE_Ratio             42\n",
      "Beta                 42\n",
      "Dividend_Yield       42\n",
      "ROE                  42\n",
      "Profit_Margin        42\n",
      "dtype: int64\n",
      "\n",
      "============================================================\n",
      "DATA BY STOCK (2022+):\n",
      "============================================================\n",
      "\n",
      "AAPL:\n",
      "  Total quarters: 14\n",
      "  Date range: 2022-03-31 00:00:00 to 2025-06-30 00:00:00\n",
      "  EPS records: 14\n",
      "  Revenue records: 12\n",
      "  Net_Income records: 12\n",
      "  Debt_to_Equity records: 12\n",
      "\n",
      "GOOGL:\n",
      "  Total quarters: 14\n",
      "  Date range: 2022-03-31 00:00:00 to 2025-06-30 00:00:00\n",
      "  EPS records: 14\n",
      "  Revenue records: 12\n",
      "  Net_Income records: 12\n",
      "  Debt_to_Equity records: 12\n",
      "\n",
      "MSFT:\n",
      "  Total quarters: 14\n",
      "  Date range: 2022-03-31 00:00:00 to 2025-06-30 00:00:00\n",
      "  EPS records: 14\n",
      "  Revenue records: 12\n",
      "  Net_Income records: 12\n",
      "  Debt_to_Equity records: 12\n",
      "\n",
      "============================================================\n",
      "SAMPLE OF COMPLETE ROWS (with Revenue):\n",
      "============================================================\n",
      "      Date Ticker  Reported_EPS      Revenue   Net_Income  Debt_to_Equity\n",
      "2022-09-30   AAPL          1.29 9.014600e+10 2.072100e+10        2.369533\n",
      "2022-09-30  GOOGL          1.06 6.909200e+10 1.391000e+10        0.066783\n",
      "2022-09-30   MSFT          2.35 5.012200e+10 1.755600e+10        0.280136\n",
      "2022-12-31   AAPL          1.88 1.171540e+11 2.999800e+10        1.958679\n",
      "2022-12-31  GOOGL          1.05 7.604800e+10 1.362400e+10        0.059865\n",
      "2022-12-31   MSFT          2.32 5.274700e+10 1.642500e+10        0.262734\n",
      "2023-03-31   AAPL          1.52 9.483600e+10 2.416000e+10        1.763490\n",
      "2023-03-31  GOOGL          1.17 6.978700e+10 1.505100e+10        0.062562\n",
      "2023-03-31   MSFT          2.45 5.285700e+10 1.829900e+10        0.247633\n",
      "2023-06-30   AAPL          1.26 8.179700e+10 1.988100e+10        1.813054\n",
      "2023-06-30  GOOGL          1.44 7.460400e+10 1.836800e+10        0.061271\n",
      "2023-06-30   MSFT          2.69 5.618900e+10 2.008100e+10        0.246544\n",
      "2023-09-30   AAPL          1.46 8.949800e+10 2.295600e+10        1.812876\n",
      "2023-09-30  GOOGL          1.55 7.669300e+10 1.968900e+10        0.060380\n",
      "2023-09-30   MSFT          2.99 5.651700e+10 2.229100e+10        0.323958\n",
      "2023-12-31   AAPL          2.18 1.195750e+11 3.391600e+10        1.458030\n",
      "2023-12-31  GOOGL          1.64 8.631000e+10 2.068700e+10        0.051736\n",
      "2023-12-31   MSFT          2.93 6.202000e+10 2.187000e+10        0.330288\n",
      "2024-03-31   AAPL          1.53 9.075300e+10 2.363600e+10        1.409683\n",
      "2024-03-31  GOOGL          1.89 8.053900e+10 2.366200e+10        0.050358\n",
      "2024-03-31   MSFT          2.94 6.185800e+10 2.193900e+10        0.308254\n",
      "2024-06-30   AAPL          1.40 8.577700e+10 2.144800e+10        1.518618\n",
      "2024-06-30  GOOGL          1.89 8.474200e+10 2.361900e+10        0.048980\n",
      "2024-06-30   MSFT          2.95 6.472700e+10 2.203600e+10        0.233078\n",
      "2024-09-30   AAPL          0.97 9.493000e+10 1.473600e+10        1.900983\n",
      "2024-09-30  GOOGL          2.12 8.826800e+10 2.630100e+10        0.044095\n",
      "2024-09-30   MSFT          3.30 6.558500e+10 2.466700e+10        0.179443\n",
      "2024-12-31   AAPL          2.40 1.243000e+11 3.633000e+10        1.449999\n",
      "2024-12-31  GOOGL          2.15 9.646900e+10 2.653600e+10        0.042358\n",
      "2024-12-31   MSFT          3.23 6.963200e+10 2.410800e+10        0.148565\n",
      "2025-03-31   AAPL          1.65 9.535900e+10 2.478000e+10        1.469938\n",
      "2025-03-31  GOOGL          2.81 9.023400e+10 3.454000e+10        0.034426\n",
      "2025-03-31   MSFT          3.46 7.006600e+10 2.582400e+10        0.133216\n",
      "2025-06-30   AAPL          1.57 9.403600e+10 2.343400e+10        1.544858\n",
      "2025-06-30  GOOGL          2.31 9.642800e+10 2.819600e+10        0.076365\n",
      "2025-06-30   MSFT          3.65 7.644100e+10 2.723300e+10        0.125629\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data/raw/fundamentals_quarterly.csv')\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "# Filter 2022+\n",
    "recent = df[df['Date'] >= '2022-01-01'].copy()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"COMPLETE 2022+ DATA CHECK\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check each column's completeness\n",
    "print(\"\\nNon-null counts for 2022+ data:\")\n",
    "print(recent.count())\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"DATA BY STOCK (2022+):\")\n",
    "print(\"=\" * 60)\n",
    "for ticker in recent['Ticker'].unique():\n",
    "    stock_data = recent[recent['Ticker'] == ticker]\n",
    "    print(f\"\\n{ticker}:\")\n",
    "    print(f\"  Total quarters: {len(stock_data)}\")\n",
    "    print(f\"  Date range: {stock_data['Date'].min()} to {stock_data['Date'].max()}\")\n",
    "    print(f\"  EPS records: {stock_data['Reported_EPS'].notna().sum()}\")\n",
    "    print(f\"  Revenue records: {stock_data['Revenue'].notna().sum()}\")\n",
    "    print(f\"  Net_Income records: {stock_data['Net_Income'].notna().sum()}\")\n",
    "    print(f\"  Debt_to_Equity records: {stock_data['Debt_to_Equity'].notna().sum()}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SAMPLE OF COMPLETE ROWS (with Revenue):\")\n",
    "print(\"=\" * 60)\n",
    "complete = recent[recent['Revenue'].notna()]\n",
    "cols = ['Date', 'Ticker', 'Reported_EPS', 'Revenue', 'Net_Income', 'Debt_to_Equity']\n",
    "print(complete[cols].to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3845c95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "PART 1: FORWARD-FILLING FUNDAMENTALS TO DAILY\n",
      "============================================================\n",
      "\n",
      "1. Loading data...\n",
      "   ✅ Loaded 16 quarterly fundamental records.\n",
      "   ✅ Loaded 11310 daily price records.\n",
      "\n",
      "2. Tickers to process: AAPL, GOOGL, MSFT\n",
      "\n",
      "3. Forward-filling for each ticker...\n",
      "   - ✅ AAPL: 3770 daily records created.\n",
      "   - ✅ GOOGL: 3770 daily records created.\n",
      "   - ✅ MSFT: 3770 daily records created.\n",
      "\n",
      "============================================================\n",
      "FUNDAMENTALS SUMMARY\n",
      "============================================================\n",
      "Total daily records: 11,310\n",
      "Date range: 2010-10-14 to 2025-10-09\n",
      "✅ Daily fundamentals saved to: data/processed/fundamentals_daily.csv\n",
      "\n",
      "============================================================\n",
      "PART 2: BACKFILLING SENTIMENT DATA WITH RANDOMIZED VALUES\n",
      "============================================================\n",
      "\n",
      "1. Loading data...\n",
      "   ✅ Loaded 70 real sentiment records.\n",
      "\n",
      "2. Tickers to process: AAPL, GOOGL, MSFT\n",
      "   - Processing AAPL...\n",
      "     - ✅ Generated 3737 synthetic records, combined with 47 real records.\n",
      "   - Processing GOOGL...\n",
      "     - No real sentiment data for GOOGL. Generating full synthetic history.\n",
      "     - ✅ Generated 3770 synthetic records, combined with 0 real records.\n",
      "   - Processing MSFT...\n",
      "     - ✅ Generated 3753 synthetic records, combined with 23 real records.\n",
      "\n",
      "============================================================\n",
      "SENTIMENT SUMMARY\n",
      "============================================================\n",
      "Total daily records (real + synthetic): 11,330\n",
      "Date range: 2010-10-14 to 2025-10-09\n",
      "✅ Backfilled sentiment data saved to: data/processed/sentiment_daily_backfilled.csv\n",
      "\n",
      "============================================================\n",
      "✅ STEP 6 COMPLETE!\n",
      "============================================================\n",
      "\n",
      "You now have two new processed files:\n",
      "   • data/processed/fundamentals_daily.csv\n",
      "   • data/processed/sentiment_daily_backfilled.csv\n",
      "\n",
      "All datasets are now on a daily frequency and ready for the final merge!\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Forward-Fill Fundamentals & Backfill Sentiment Data\n",
    "# This script prepares the final datasets for merging.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def forward_fill_fundamentals():\n",
    "    \"\"\"\n",
    "    Forward-fills quarterly fundamentals to a daily frequency using the \n",
    "    date range from the processed price data.\n",
    "    Saves to: data/processed/fundamentals_daily.csv\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"PART 1: FORWARD-FILLING FUNDAMENTALS TO DAILY\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # --- Load Data ---\n",
    "    print(\"\\n1. Loading data...\")\n",
    "    try:\n",
    "        fund_df = pd.read_csv('data/raw/fundamentals_quarterly.csv')\n",
    "        price_df = pd.read_csv('data/processed/stock_prices_with_metrics.csv')\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"❌ ERROR: Missing required file: {e}. Please run previous steps.\")\n",
    "        return\n",
    "\n",
    "    # --- FIX: Add check for empty fundamentals file ---\n",
    "    if fund_df.empty:\n",
    "        print(\"   ❌ ERROR: 'fundamentals_quarterly.csv' is empty. Cannot proceed.\")\n",
    "        print(\"   Please re-run the script that generates this file (Step 3).\")\n",
    "        return\n",
    "\n",
    "    # --- Prepare Dates ---\n",
    "    fund_df['Date'] = pd.to_datetime(fund_df['Date']).dt.normalize()\n",
    "    price_df['Date'] = pd.to_datetime(price_df['Date']).dt.normalize()\n",
    "    \n",
    "    print(f\"   ✅ Loaded {len(fund_df)} quarterly fundamental records.\")\n",
    "    print(f\"   ✅ Loaded {len(price_df)} daily price records.\")\n",
    "    \n",
    "    fund_tickers = fund_df['Ticker'].unique()\n",
    "    print(f\"\\n2. Tickers to process: {', '.join(fund_tickers)}\")\n",
    "    \n",
    "    # --- Process Each Ticker ---\n",
    "    print(\"\\n3. Forward-filling for each ticker...\")\n",
    "    all_daily_fundamentals = []\n",
    "    \n",
    "    for ticker in fund_tickers:\n",
    "        # Get all trading dates for this ticker from the price data\n",
    "        ticker_dates_df = price_df[price_df['Ticker'] == ticker][['Date', 'Ticker']].copy().sort_values('Date').drop_duplicates()\n",
    "        \n",
    "        # Get fundamentals for this ticker\n",
    "        ticker_fund_df = fund_df[fund_df['Ticker'] == ticker].copy().sort_values('Date')\n",
    "        \n",
    "        if ticker_fund_df.empty:\n",
    "            print(f\"   - ⚠️  Warning: No fundamental data found for {ticker}. Skipping.\")\n",
    "            continue\n",
    "            \n",
    "        # Use merge_asof to find the last known fundamental data for each trading day\n",
    "        daily_filled_df = pd.merge_asof(\n",
    "            ticker_dates_df,\n",
    "            ticker_fund_df,\n",
    "            on='Date',\n",
    "            by='Ticker',\n",
    "            direction='backward' # Use the most recent quarterly report\n",
    "        )\n",
    "        \n",
    "        all_daily_fundamentals.append(daily_filled_df)\n",
    "        print(f\"   - ✅ {ticker}: {len(daily_filled_df)} daily records created.\")\n",
    "    \n",
    "    if not all_daily_fundamentals:\n",
    "        print(\"\\n❌ No daily fundamental data was generated.\")\n",
    "        return\n",
    "        \n",
    "    # --- Combine and Save ---\n",
    "    final_df = pd.concat(all_daily_fundamentals, ignore_index=True)\n",
    "    final_df = final_df.sort_values(['Date', 'Ticker']).reset_index(drop=True)\n",
    "    \n",
    "    # --- FIX: Add check for empty final dataframe before printing summary ---\n",
    "    if final_df.empty or final_df['Date'].isnull().all():\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"FUNDAMENTALS SUMMARY\")\n",
    "        print(\"=\" * 60)\n",
    "        print(\"   ❌ ERROR: The final fundamentals dataframe is empty or contains no valid dates.\")\n",
    "        print(\"   This can happen if there are no overlapping tickers between price and fundamentals data.\")\n",
    "        return\n",
    "\n",
    "    output_file = 'data/processed/fundamentals_daily.csv'\n",
    "    final_df.to_csv(output_file, index=False)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"FUNDAMENTALS SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Total daily records: {len(final_df):,}\")\n",
    "    print(f\"Date range: {final_df['Date'].min().strftime('%Y-%m-%d')} to {final_df['Date'].max().strftime('%Y-%m-%d')}\")\n",
    "    print(f\"✅ Daily fundamentals saved to: {output_file}\")\n",
    "\n",
    "\n",
    "def backfill_sentiment_data():\n",
    "    \"\"\"\n",
    "    Fills historical gaps in sentiment data with plausible random values,\n",
    "    then combines it with the real, recent data.\n",
    "    Saves to: data/processed/sentiment_daily_backfilled.csv\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"PART 2: BACKFILLING SENTIMENT DATA WITH RANDOMIZED VALUES\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # --- Load Data ---\n",
    "    print(\"\\n1. Loading data...\")\n",
    "    try:\n",
    "        sentiment_df = pd.read_csv('data/raw/sentiment_news_data.csv')\n",
    "        price_df = pd.read_csv('data/processed/stock_prices_with_metrics.csv')\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"❌ ERROR: Missing required file: {e}. Please run previous steps.\")\n",
    "        return\n",
    "\n",
    "    # --- Prepare Dates ---\n",
    "    sentiment_df['Date'] = pd.to_datetime(sentiment_df['Date']).dt.normalize()\n",
    "    price_df['Date'] = pd.to_datetime(price_df['Date']).dt.normalize()\n",
    "    print(f\"   ✅ Loaded {len(sentiment_df)} real sentiment records.\")\n",
    "\n",
    "    tickers = price_df['Ticker'].unique()\n",
    "    print(f\"\\n2. Tickers to process: {', '.join(tickers)}\")\n",
    "    \n",
    "    all_full_sentiment = []\n",
    "\n",
    "    for ticker in tickers:\n",
    "        print(f\"   - Processing {ticker}...\")\n",
    "        \n",
    "        # --- Get Date Ranges ---\n",
    "        ticker_dates_df = price_df[price_df['Ticker'] == ticker][['Date']].copy().sort_values('Date').drop_duplicates()\n",
    "        ticker_real_sentiment = sentiment_df[sentiment_df['Ticker'] == ticker].copy()\n",
    "\n",
    "        if ticker_real_sentiment.empty:\n",
    "            print(f\"     - No real sentiment data for {ticker}. Generating full synthetic history.\")\n",
    "            first_real_date = ticker_dates_df['Date'].max() + pd.Timedelta(days=1)\n",
    "        else:\n",
    "            first_real_date = ticker_real_sentiment['Date'].min()\n",
    "\n",
    "        # --- Generate Synthetic Historical Data ---\n",
    "        historical_dates = ticker_dates_df[ticker_dates_df['Date'] < first_real_date].copy()\n",
    "        \n",
    "        if not historical_dates.empty:\n",
    "            # Create a random walk for more realistic sentiment drift\n",
    "            start_value = np.random.uniform(-0.1, 0.1) # Start near neutral\n",
    "            steps = np.random.normal(loc=0.0, scale=0.03, size=len(historical_dates))\n",
    "            random_walk = np.cumsum(steps) + start_value\n",
    "            \n",
    "            # Clip values to a plausible range (e.g., -0.5 to 0.5 for sentiment)\n",
    "            synthetic_scores = np.clip(random_walk, -0.5, 0.5)\n",
    "            \n",
    "            historical_dates['Ticker'] = ticker\n",
    "            historical_dates['trend_score'] = synthetic_scores\n",
    "            historical_dates['sentiment_volume'] = 0 # Indicate synthetic data\n",
    "            \n",
    "            # --- Combine Synthetic and Real Data ---\n",
    "            combined_df = pd.concat([historical_dates, ticker_real_sentiment], ignore_index=True)\n",
    "            all_full_sentiment.append(combined_df)\n",
    "            print(f\"     - ✅ Generated {len(historical_dates)} synthetic records, combined with {len(ticker_real_sentiment)} real records.\")\n",
    "        else:\n",
    "            # If there are no historical dates, just use the real data\n",
    "            all_full_sentiment.append(ticker_real_sentiment)\n",
    "            print(f\"     - ✅ No backfilling needed, using {len(ticker_real_sentiment)} real records.\")\n",
    "\n",
    "    if not all_full_sentiment:\n",
    "        print(\"\\n❌ No sentiment data was generated.\")\n",
    "        return\n",
    "\n",
    "    # --- Combine All Tickers and Save ---\n",
    "    final_df = pd.concat(all_full_sentiment, ignore_index=True)\n",
    "    final_df = final_df.sort_values(['Date', 'Ticker']).reset_index(drop=True)\n",
    "    \n",
    "    # --- FIX: Add check for empty final dataframe before printing summary ---\n",
    "    if final_df.empty or final_df['Date'].isnull().all():\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"SENTIMENT SUMMARY\")\n",
    "        print(\"=\" * 60)\n",
    "        print(\"   ❌ ERROR: The final sentiment dataframe is empty or contains no valid dates.\")\n",
    "        return\n",
    "\n",
    "    output_file = 'data/processed/sentiment_daily_backfilled.csv'\n",
    "    final_df.to_csv(output_file, index=False)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"SENTIMENT SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Total daily records (real + synthetic): {len(final_df):,}\")\n",
    "    print(f\"Date range: {final_df['Date'].min().strftime('%Y-%m-%d')} to {final_df['Date'].max().strftime('%Y-%m-%d')}\")\n",
    "    print(f\"✅ Backfilled sentiment data saved to: {output_file}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Part 1: Process the fundamentals data\n",
    "    forward_fill_fundamentals()\n",
    "    \n",
    "    # Part 2: Process the sentiment data\n",
    "    backfill_sentiment_data()\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"✅ STEP 6 COMPLETE!\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"\\nYou now have two new processed files:\")\n",
    "    print(\"   • data/processed/fundamentals_daily.csv\")\n",
    "    print(\"   • data/processed/sentiment_daily_backfilled.csv\")\n",
    "    print(\"\\nAll datasets are now on a daily frequency and ready for the final merge!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36405e5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "PROCESSING MACRO DATA (FOR FULL 15-YEAR HISTORY)\n",
      "============================================================\n",
      "\n",
      "1. Loading raw data...\n",
      "   ✅ Loaded 5487 raw macro records.\n",
      "   ✅ Loaded 11310 processed price records.\n",
      "\n",
      "2. Aligning macro data to stock trading days...\n",
      "   - Found 3770 unique trading days.\n",
      "\n",
      "3. Filling gaps to create a complete daily dataset...\n",
      "   ✅ Gaps filled successfully.\n",
      "\n",
      "============================================================\n",
      "SUMMARY\n",
      "============================================================\n",
      "Total daily records created: 3,770\n",
      "Date range: 2010-10-14 to 2025-10-09\n",
      "✅ Daily macro data saved to: data/processed/macro_data_daily.csv\n",
      "\n",
      "============================================================\n",
      "DATA QUALITY CHECK\n",
      "============================================================\n",
      "   ✅ No missing values found!\n",
      "\n",
      "============================================================\n",
      "✅ MACRO PROCESSING COMPLETE!\n",
      "============================================================\n",
      "\n",
      "You now have daily macro data covering the full 15-year period.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yashs\\AppData\\Local\\Temp\\ipykernel_6972\\801031506.py:49: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df_daily = df_daily.fillna(method='ffill')\n",
      "C:\\Users\\yashs\\AppData\\Local\\Temp\\ipykernel_6972\\801031506.py:50: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df_daily = df_daily.fillna(method='bfill')\n"
     ]
    }
   ],
   "source": [
    "# Process Macro Data - Forward-fill for the entire 15-year history\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def process_macro_data():\n",
    "    \"\"\"\n",
    "    Loads the full history of raw macro data, aligns it with the daily\n",
    "    stock trading dates, and forward-fills any gaps.\n",
    "    Saves to: data/processed/macro_data_daily.csv\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"PROCESSING MACRO DATA (FOR FULL 15-YEAR HISTORY)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # --- Load Data ---\n",
    "    print(\"\\n1. Loading raw data...\")\n",
    "    try:\n",
    "        df = pd.read_csv('data/raw/macro_data_raw.csv')\n",
    "        price_df = pd.read_csv('data/processed/stock_prices_with_metrics.csv')\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"   ❌ ERROR: Missing required file: {e}. Please run the earlier scripts first.\")\n",
    "        return None\n",
    "        \n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    price_df['Date'] = pd.to_datetime(price_df['Date'])\n",
    "    \n",
    "    print(f\"   ✅ Loaded {len(df)} raw macro records.\")\n",
    "    print(f\"   ✅ Loaded {len(price_df)} processed price records.\")\n",
    "    \n",
    "    # --- Align to Price Data Dates ---\n",
    "    print(\"\\n2. Aligning macro data to stock trading days...\")\n",
    "    \n",
    "    # Get all unique trading days from the price data\n",
    "    price_dates = sorted(price_df['Date'].unique())\n",
    "    df_daily = pd.DataFrame({'Date': price_dates})\n",
    "    \n",
    "    print(f\"   - Found {len(price_dates)} unique trading days.\")\n",
    "    \n",
    "    # Merge macro data onto the trading day calendar.\n",
    "    # 'how=left' keeps all trading days, even if there's no macro data for that specific day.\n",
    "    df_daily = pd.merge(df_daily, df, on='Date', how='left')\n",
    "    \n",
    "    # --- Forward-Fill and Back-Fill ---\n",
    "    # Forward-fill carries the last known value forward (e.g., monthly CPI applies to all subsequent days).\n",
    "    # Back-fill handles any missing values at the very beginning of the dataset.\n",
    "    print(\"\\n3. Filling gaps to create a complete daily dataset...\")\n",
    "    df_daily = df_daily.fillna(method='ffill')\n",
    "    df_daily = df_daily.fillna(method='bfill')\n",
    "    print(\"   ✅ Gaps filled successfully.\")\n",
    "    \n",
    "    # --- Save Processed Data ---\n",
    "    os.makedirs('data/processed', exist_ok=True)\n",
    "    output_file = 'data/processed/macro_data_daily.csv'\n",
    "    df_daily.to_csv(output_file, index=False)\n",
    "    \n",
    "    # --- Summary ---\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Total daily records created: {len(df_daily):,}\")\n",
    "    print(f\"Date range: {df_daily['Date'].min().strftime('%Y-%m-%d')} to {df_daily['Date'].max().strftime('%Y-%m-%d')}\")\n",
    "    print(f\"✅ Daily macro data saved to: {output_file}\")\n",
    "    \n",
    "    # --- Data Quality Check ---\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"DATA QUALITY CHECK\")\n",
    "    print(\"=\" * 60)\n",
    "    missing_values = df_daily.isnull().sum().sum()\n",
    "    if missing_values == 0:\n",
    "        print(\"   ✅ No missing values found!\")\n",
    "    else:\n",
    "        print(f\"   ❌ Warning: {missing_values} missing values remain.\")\n",
    "\n",
    "    return df_daily\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    df = process_macro_data()\n",
    "    \n",
    "    if df is not None:\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"✅ MACRO PROCESSING COMPLETE!\")\n",
    "        print(\"=\" * 60)\n",
    "        print(\"\\nYou now have daily macro data covering the full 15-year period.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f4b1832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "DATA QUALITY CHECK - MASTER DATASET\n",
      "======================================================================\n",
      "\n",
      "1. Loading master dataset...\n",
      "   ✅ Loaded: 2,253 records, 49 columns\n",
      "\n",
      "======================================================================\n",
      "2. BASIC STRUCTURE\n",
      "======================================================================\n",
      "Total Records: 2,253\n",
      "Total Columns: 49\n",
      "Date Range: 2022-09-30 00:00:00 to 2025-09-29 00:00:00\n",
      "Trading Days: 751\n",
      "Tickers: AAPL, GOOGL, MSFT\n",
      "Records per Ticker:\n",
      "Ticker\n",
      "AAPL     751\n",
      "GOOGL    751\n",
      "MSFT     751\n",
      "dtype: int64\n",
      "\n",
      "======================================================================\n",
      "3. MISSING VALUES ANALYSIS\n",
      "======================================================================\n",
      "\n",
      "Columns with missing values:\n",
      "                    Missing_Count  Missing_Pct\n",
      "MA_200                        297        13.18\n",
      "MA_50                          72         3.20\n",
      "Realized_Vol_20d               30         1.33\n",
      "MA_20                          27         1.20\n",
      "Volume_MA_20                   27         1.20\n",
      "Volume_Ratio                   27         1.20\n",
      "Momentum_5d                    15         0.67\n",
      "Realized_Vol_10d               15         0.67\n",
      "Target_Vol_Next                15         0.67\n",
      "Return_Pct                      3         0.13\n",
      "Target_Price_Next               3         0.13\n",
      "Target_Return_Next              3         0.13\n",
      "\n",
      "======================================================================\n",
      "4. DATA TYPES\n",
      "======================================================================\n",
      "\n",
      "Column data types:\n",
      "float64           45\n",
      "int64              2\n",
      "object             1\n",
      "datetime64[ns]     1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "======================================================================\n",
      "5. DUPLICATE CHECK\n",
      "======================================================================\n",
      "Duplicate (Date, Ticker) pairs: 0\n",
      "✅ No duplicates found\n",
      "\n",
      "======================================================================\n",
      "6. DATE CONTINUITY CHECK\n",
      "======================================================================\n",
      "AAPL: ✅ Continuous dates\n",
      "GOOGL: ✅ Continuous dates\n",
      "MSFT: ✅ Continuous dates\n",
      "\n",
      "======================================================================\n",
      "7. VALUE RANGE CHECK (Key Columns)\n",
      "======================================================================\n",
      "\n",
      "Close:\n",
      "  Min: 82.86\n",
      "  Max: 534.76\n",
      "  Mean: 237.60\n",
      "  Std: 111.70\n",
      "\n",
      "Volume:\n",
      "  Min: 1222536.00\n",
      "  Max: 318679900.00\n",
      "  Mean: 38669062.00\n",
      "  Std: 23648012.87\n",
      "  ⚠️  Outliers (>3 std): 34 values\n",
      "\n",
      "Return_Pct:\n",
      "  Min: -9.51\n",
      "  Max: 15.33\n",
      "  Mean: 0.12\n",
      "  Std: 1.78\n",
      "  ⚠️  Outliers (>3 std): 31 values\n",
      "\n",
      "Realized_Vol_20d:\n",
      "  Min: 0.60\n",
      "  Max: 5.15\n",
      "  Mean: 1.67\n",
      "  Std: 0.67\n",
      "  ⚠️  Outliers (>3 std): 29 values\n",
      "\n",
      "Revenue:\n",
      "  Min: 50122000000.00\n",
      "  Max: 124300000000.00\n",
      "  Mean: 80626789170.00\n",
      "  Std: 18132835433.52\n",
      "\n",
      "Fed_Funds_Rate:\n",
      "  Min: 3.08\n",
      "  Max: 5.33\n",
      "  Mean: 4.77\n",
      "  Std: 0.56\n",
      "  ⚠️  Outliers (>3 std): 72 values\n",
      "\n",
      "CPI:\n",
      "  Min: 296.42\n",
      "  Max: 323.36\n",
      "  Mean: 311.24\n",
      "  Std: 7.65\n",
      "\n",
      "VIX:\n",
      "  Min: 11.86\n",
      "  Max: 52.33\n",
      "  Mean: 17.77\n",
      "  Std: 4.91\n",
      "  ⚠️  Outliers (>3 std): 33 values\n",
      "\n",
      "======================================================================\n",
      "8. TARGET VARIABLES CHECK (for ML)\n",
      "======================================================================\n",
      "\n",
      "Target_Price_Next:\n",
      "  Available: 2,250 (99.9%)\n",
      "  Missing: 3 (0.1%)\n",
      "\n",
      "Target_Return_Next:\n",
      "  Available: 2,250 (99.9%)\n",
      "  Missing: 3 (0.1%)\n",
      "\n",
      "Target_Vol_Next:\n",
      "  Available: 2,238 (99.3%)\n",
      "  Missing: 15 (0.7%)\n",
      "\n",
      "======================================================================\n",
      "9. FEATURE COMPLETENESS BY CATEGORY\n",
      "======================================================================\n",
      "\n",
      "Price: 5/5 columns available\n",
      "  Complete rows: 100.0%\n",
      "\n",
      "Technical: 4/4 columns available\n",
      "  Complete rows: 86.8%\n",
      "\n",
      "Fundamentals: 4/4 columns available\n",
      "  Complete rows: 100.0%\n",
      "\n",
      "Macro: 4/4 columns available\n",
      "  Complete rows: 100.0%\n",
      "\n",
      "Sentiment: 1/1 columns available\n",
      "  Complete rows: 100.0%\n",
      "\n",
      "======================================================================\n",
      "10. HIGH CORRELATION CHECK (Multicollinearity)\n",
      "======================================================================\n",
      "\n",
      "⚠️  Highly correlated pairs (>0.9):\n",
      "  Open <-> High: 1.000\n",
      "  Open <-> Low: 1.000\n",
      "  Open <-> Close: 1.000\n",
      "  Open <-> MA_20: 0.997\n",
      "  Open <-> MA_50: 0.991\n",
      "  Open <-> MA_200: 0.976\n",
      "  Open <-> Target_Price_Next: 0.999\n",
      "  High <-> Low: 1.000\n",
      "  High <-> Close: 1.000\n",
      "  High <-> MA_20: 0.997\n",
      "\n",
      "======================================================================\n",
      "11. ML READINESS ASSESSMENT\n",
      "======================================================================\n",
      "\n",
      "⚠️  Issues detected:\n",
      "  • Missing values in Return_Pct\n",
      "  • Missing values in Realized_Vol_20d\n",
      "\n",
      "======================================================================\n",
      "SUMMARY\n",
      "======================================================================\n",
      "Complete Columns: 37/49 (75.5%)\n",
      "Complete Rows: 1,953/2,253 (86.7%)\n",
      "Memory Usage: 0.94 MB\n",
      "\n",
      "======================================================================\n",
      "✅ QUALITY CHECK COMPLETE!\n",
      "======================================================================\n",
      "\n",
      "Recommendations:\n",
      "  1. Review any warnings above\n",
      "  2. Handle missing values if needed\n",
      "  3. Remove or combine highly correlated features\n",
      "  4. Ready to start modeling!\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Comprehensive Data Quality Check\n",
    "# Validates master dataset for completeness, consistency, and ML readiness\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def check_data_quality():\n",
    "    \"\"\"\n",
    "    Comprehensive quality check of master dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"DATA QUALITY CHECK - MASTER DATASET\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Load master dataset\n",
    "    print(\"\\n1. Loading master dataset...\")\n",
    "    try:\n",
    "        df = pd.read_csv('data/features/master_dataset_clean.csv')\n",
    "        df['Date'] = pd.to_datetime(df['Date'])\n",
    "        print(f\"   ✅ Loaded: {len(df):,} records, {len(df.columns)} columns\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"   ❌ Master dataset not found! Run step6_combine_all_data.py first\")\n",
    "        return None\n",
    "    \n",
    "    # Basic Structure Check\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"2. BASIC STRUCTURE\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Total Records: {len(df):,}\")\n",
    "    print(f\"Total Columns: {len(df.columns)}\")\n",
    "    print(f\"Date Range: {df['Date'].min()} to {df['Date'].max()}\")\n",
    "    print(f\"Trading Days: {df['Date'].nunique()}\")\n",
    "    print(f\"Tickers: {', '.join(sorted(df['Ticker'].unique()))}\")\n",
    "    print(f\"Records per Ticker:\")\n",
    "    print(df.groupby('Ticker').size())\n",
    "    \n",
    "    # Missing Values Analysis\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"3. MISSING VALUES ANALYSIS\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    missing_counts = df.isnull().sum()\n",
    "    missing_pct = (missing_counts / len(df) * 100).round(2)\n",
    "    missing_df = pd.DataFrame({\n",
    "        'Missing_Count': missing_counts,\n",
    "        'Missing_Pct': missing_pct\n",
    "    })\n",
    "    missing_df = missing_df[missing_df['Missing_Count'] > 0].sort_values('Missing_Count', ascending=False)\n",
    "    \n",
    "    if len(missing_df) > 0:\n",
    "        print(\"\\nColumns with missing values:\")\n",
    "        print(missing_df.to_string())\n",
    "    else:\n",
    "        print(\"\\n✅ No missing values detected!\")\n",
    "    \n",
    "    # Data Type Check\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"4. DATA TYPES\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"\\nColumn data types:\")\n",
    "    print(df.dtypes.value_counts())\n",
    "    \n",
    "    # Duplicate Check\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"5. DUPLICATE CHECK\")\n",
    "    print(\"=\" * 70)\n",
    "    duplicates = df.duplicated(subset=['Date', 'Ticker']).sum()\n",
    "    print(f\"Duplicate (Date, Ticker) pairs: {duplicates}\")\n",
    "    if duplicates > 0:\n",
    "        print(\"⚠️  WARNING: Found duplicates!\")\n",
    "    else:\n",
    "        print(\"✅ No duplicates found\")\n",
    "    \n",
    "    # Date Continuity Check\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"6. DATE CONTINUITY CHECK\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    for ticker in df['Ticker'].unique():\n",
    "        ticker_df = df[df['Ticker'] == ticker].sort_values('Date')\n",
    "        date_diff = ticker_df['Date'].diff()\n",
    "        gaps = date_diff[date_diff > pd.Timedelta(days=5)]  # More than 5 days gap\n",
    "        \n",
    "        if len(gaps) > 0:\n",
    "            print(f\"\\n{ticker}: {len(gaps)} gaps found (>5 days)\")\n",
    "        else:\n",
    "            print(f\"{ticker}: ✅ Continuous dates\")\n",
    "    \n",
    "    # Value Range Check (detect outliers)\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"7. VALUE RANGE CHECK (Key Columns)\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    numeric_cols = ['Close', 'Volume', 'Return_Pct', 'Realized_Vol_20d', \n",
    "                    'Revenue', 'Fed_Funds_Rate', 'CPI', 'VIX']\n",
    "    \n",
    "    for col in numeric_cols:\n",
    "        if col in df.columns:\n",
    "            col_data = df[col].dropna()\n",
    "            if len(col_data) > 0:\n",
    "                print(f\"\\n{col}:\")\n",
    "                print(f\"  Min: {col_data.min():.2f}\")\n",
    "                print(f\"  Max: {col_data.max():.2f}\")\n",
    "                print(f\"  Mean: {col_data.mean():.2f}\")\n",
    "                print(f\"  Std: {col_data.std():.2f}\")\n",
    "                \n",
    "                # Check for extreme outliers (beyond 3 std)\n",
    "                mean = col_data.mean()\n",
    "                std = col_data.std()\n",
    "                outliers = col_data[(col_data < mean - 3*std) | (col_data > mean + 3*std)]\n",
    "                if len(outliers) > 0:\n",
    "                    print(f\"  ⚠️  Outliers (>3 std): {len(outliers)} values\")\n",
    "    \n",
    "    # Target Variables Check\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"8. TARGET VARIABLES CHECK (for ML)\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    target_cols = ['Target_Price_Next', 'Target_Return_Next', 'Target_Vol_Next']\n",
    "    for col in target_cols:\n",
    "        if col in df.columns:\n",
    "            non_null = df[col].notna().sum()\n",
    "            null_count = df[col].isna().sum()\n",
    "            print(f\"\\n{col}:\")\n",
    "            print(f\"  Available: {non_null:,} ({non_null/len(df)*100:.1f}%)\")\n",
    "            print(f\"  Missing: {null_count:,} ({null_count/len(df)*100:.1f}%)\")\n",
    "    \n",
    "    # Feature Completeness by Category\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"9. FEATURE COMPLETENESS BY CATEGORY\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    categories = {\n",
    "        'Price': ['Open', 'High', 'Low', 'Close', 'Volume'],\n",
    "        'Technical': ['Return_Pct', 'Realized_Vol_20d', 'MA_50', 'MA_200'],\n",
    "        'Fundamentals': ['Reported_EPS', 'Revenue', 'Net_Income', 'Debt_to_Equity'],\n",
    "        'Macro': ['Fed_Funds_Rate', 'CPI', 'VIX', 'GDP'],\n",
    "        'Sentiment': ['Trend_Score']\n",
    "    }\n",
    "    \n",
    "    for category, cols in categories.items():\n",
    "        available = [c for c in cols if c in df.columns]\n",
    "        if available:\n",
    "            completeness = df[available].notna().all(axis=1).sum() / len(df) * 100\n",
    "            print(f\"\\n{category}: {len(available)}/{len(cols)} columns available\")\n",
    "            print(f\"  Complete rows: {completeness:.1f}%\")\n",
    "    \n",
    "    # Correlation Check (multicollinearity warning)\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"10. HIGH CORRELATION CHECK (Multicollinearity)\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    numeric_df = df.select_dtypes(include=[np.number])\n",
    "    corr_matrix = numeric_df.corr().abs()\n",
    "    \n",
    "    # Find highly correlated pairs (>0.9)\n",
    "    high_corr = []\n",
    "    for i in range(len(corr_matrix.columns)):\n",
    "        for j in range(i+1, len(corr_matrix.columns)):\n",
    "            if corr_matrix.iloc[i, j] > 0.9:\n",
    "                high_corr.append((corr_matrix.columns[i], corr_matrix.columns[j], corr_matrix.iloc[i, j]))\n",
    "    \n",
    "    if high_corr:\n",
    "        print(\"\\n⚠️  Highly correlated pairs (>0.9):\")\n",
    "        for col1, col2, corr_val in high_corr[:10]:  # Show top 10\n",
    "            print(f\"  {col1} <-> {col2}: {corr_val:.3f}\")\n",
    "    else:\n",
    "        print(\"\\n✅ No extreme multicollinearity detected\")\n",
    "    \n",
    "    # Final Readiness Assessment\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"11. ML READINESS ASSESSMENT\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    issues = []\n",
    "    \n",
    "    # Check 1: Missing values in key columns\n",
    "    key_cols = ['Close', 'Return_Pct', 'Realized_Vol_20d']\n",
    "    for col in key_cols:\n",
    "        if col in df.columns and df[col].isna().sum() > 0:\n",
    "            issues.append(f\"Missing values in {col}\")\n",
    "    \n",
    "    # Check 2: Target variables\n",
    "    if 'Target_Return_Next' in df.columns:\n",
    "        if df['Target_Return_Next'].isna().sum() / len(df) > 0.1:\n",
    "            issues.append(\"More than 10% missing target values\")\n",
    "    \n",
    "    # Check 3: Sufficient data per ticker\n",
    "    for ticker in df['Ticker'].unique():\n",
    "        ticker_count = len(df[df['Ticker'] == ticker])\n",
    "        if ticker_count < 200:\n",
    "            issues.append(f\"{ticker} has only {ticker_count} records (recommended: 200+)\")\n",
    "    \n",
    "    if issues:\n",
    "        print(\"\\n⚠️  Issues detected:\")\n",
    "        for issue in issues:\n",
    "            print(f\"  • {issue}\")\n",
    "    else:\n",
    "        print(\"\\n✅ Dataset is ready for Machine Learning!\")\n",
    "    \n",
    "    # Summary\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"SUMMARY\")\n",
    "    print(\"=\" * 70)\n",
    "    total_cols = len(df.columns)\n",
    "    complete_cols = len(df.columns[df.notna().all()])\n",
    "    print(f\"Complete Columns: {complete_cols}/{total_cols} ({complete_cols/total_cols*100:.1f}%)\")\n",
    "    print(f\"Complete Rows: {df.notna().all(axis=1).sum():,}/{len(df):,} ({df.notna().all(axis=1).sum()/len(df)*100:.1f}%)\")\n",
    "    print(f\"Memory Usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    df = check_data_quality()\n",
    "    \n",
    "    if df is not None:\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"✅ QUALITY CHECK COMPLETE!\")\n",
    "        print(\"=\" * 70)\n",
    "        print(\"\\nRecommendations:\")\n",
    "        print(\"  1. Review any warnings above\")\n",
    "        print(\"  2. Handle missing values if needed\")\n",
    "        print(\"  3. Remove or combine highly correlated features\")\n",
    "        print(\"  4. Ready to start modeling!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b44009f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CREATING FRAMEWORK DATASET\n",
      "============================================================\n",
      "\n",
      "Original dataset: 2253 records, 49 columns\n",
      "\n",
      "============================================================\n",
      "FRAMEWORK DATASET CREATED\n",
      "============================================================\n",
      "Records: 2,253\n",
      "Columns: 16\n",
      "Tickers: AAPL, GOOGL, MSFT\n",
      "Date range: 2022-09-30 00:00:00 to 2025-09-29 00:00:00\n",
      "\n",
      "✅ Saved to: data/features/framework_dataset.csv\n",
      "\n",
      "============================================================\n",
      "COLUMNS IN ORDER\n",
      "============================================================\n",
      " 1. Date\n",
      " 2. Ticker\n",
      " 3. Close\n",
      " 4. Volume\n",
      " 5. Return_Pct\n",
      " 6. Realized_Vol_20d\n",
      " 7. EPS\n",
      " 8. PE_Ratio\n",
      " 9. Debt_to_Equity\n",
      "10. Revenue\n",
      "11. Fed_Funds_Rate\n",
      "12. CPI\n",
      "13. VIX\n",
      "14. GoogleTrends\n",
      "15. Target_Price\n",
      "16. Target_Vol\n",
      "\n",
      "============================================================\n",
      "PREVIEW (First 5 records)\n",
      "============================================================\n",
      "      Date Ticker      Close  Return_Pct  EPS      Revenue  Fed_Funds_Rate  GoogleTrends\n",
      "2022-09-30   AAPL 136.052338         NaN 1.29 9.014600e+10            3.08            45\n",
      "2022-09-30  GOOGL  94.994812         NaN 1.06 6.909200e+10            3.08            53\n",
      "2022-09-30   MSFT 227.246918         NaN 2.35 5.012200e+10            3.08            26\n",
      "2022-10-03   AAPL 140.236282    3.075246 1.29 9.014600e+10            3.08            38\n",
      "2022-10-03  GOOGL  97.964340    3.125990 1.06 6.909200e+10            3.08            51\n",
      "\n",
      "============================================================\n",
      "DATA QUALITY\n",
      "============================================================\n",
      "\n",
      "Missing values:\n",
      "Return_Pct           3\n",
      "Realized_Vol_20d    30\n",
      "Target_Price         3\n",
      "Target_Vol          15\n",
      "\n",
      "============================================================\n",
      "SAMPLE ROW (Like Framework)\n",
      "============================================================\n",
      "Date                : 2022-09-30 00:00:00\n",
      "Ticker              : AAPL\n",
      "Close               : 136.05\n",
      "Volume              : 124925300\n",
      "EPS                 : 1.29\n",
      "PE_Ratio            : 38.76\n",
      "Debt_to_Equity      : 2.37\n",
      "Revenue             : 90146000000.00\n",
      "Fed_Funds_Rate      : 3.08\n",
      "CPI                 : 296.42\n",
      "VIX                 : 31.62\n",
      "GoogleTrends        : 45\n",
      "Target_Price        : 140.24\n",
      "\n",
      "============================================================\n",
      "✅ FRAMEWORK DATASET READY!\n",
      "============================================================\n",
      "\n",
      "This matches your original framework structure.\n",
      "Use this for:\n",
      "  → Excel analysis\n",
      "  → Power BI dashboards\n",
      "  → Quick exploration\n",
      "\n",
      "File: data/features/analysis_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "# Create Framework Dataset - Matches Your Original Plan\n",
    "# Extracts only the columns from your framework table\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def create_framework_dataset():\n",
    "    \"\"\"\n",
    "    Create dataset matching your framework structure\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"CREATING FRAMEWORK DATASET\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Load master dataset\n",
    "    df = pd.read_csv('data/features/master_dataset_clean.csv')\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    \n",
    "    print(f\"\\nOriginal dataset: {len(df)} records, {len(df.columns)} columns\")\n",
    "    \n",
    "    # Define columns from your framework image\n",
    "    framework_cols = [\n",
    "        # Basic Info\n",
    "        'Date',\n",
    "        'Ticker',\n",
    "        \n",
    "        # Price Data\n",
    "        'Close',\n",
    "        'Volume',\n",
    "        \n",
    "        # Technical Indicators\n",
    "        'Return_Pct',\n",
    "        'Realized_Vol_20d',\n",
    "        \n",
    "        # Fundamentals\n",
    "        'Reported_EPS',\n",
    "        'PE_Ratio',\n",
    "        'Debt_to_Equity',\n",
    "        'Revenue',\n",
    "        \n",
    "        # Macro\n",
    "        'Fed_Funds_Rate',\n",
    "        'CPI',\n",
    "        'VIX',\n",
    "        \n",
    "        # Sentiment\n",
    "        'Trend_Score',\n",
    "        \n",
    "        # Targets (for ML)\n",
    "        'Target_Price_Next',\n",
    "        'Target_Vol_Next'\n",
    "    ]\n",
    "    \n",
    "    # Check which columns exist\n",
    "    available_cols = [col for col in framework_cols if col in df.columns]\n",
    "    missing_cols = [col for col in framework_cols if col not in df.columns]\n",
    "    \n",
    "    if missing_cols:\n",
    "        print(f\"\\n⚠️  Missing columns: {', '.join(missing_cols)}\")\n",
    "    \n",
    "    # Create framework dataset\n",
    "    framework_df = df[available_cols].copy()\n",
    "    \n",
    "    # Rename to match framework (if needed)\n",
    "    rename_map = {\n",
    "        'Reported_EPS': 'EPS',\n",
    "        'Trend_Score': 'GoogleTrends',\n",
    "        'Target_Price_Next': 'Target_Price',\n",
    "        'Target_Vol_Next': 'Target_Vol'\n",
    "    }\n",
    "    \n",
    "    framework_df = framework_df.rename(columns=rename_map)\n",
    "    \n",
    "    # Sort by date and ticker\n",
    "    framework_df = framework_df.sort_values(['Date', 'Ticker']).reset_index(drop=True)\n",
    "    \n",
    "    # Save\n",
    "    output_file = 'data/features/framework_dataset.csv'\n",
    "    framework_df.to_csv(output_file, index=False)\n",
    "    \n",
    "    # Summary\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"FRAMEWORK DATASET CREATED\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Records: {len(framework_df):,}\")\n",
    "    print(f\"Columns: {len(framework_df.columns)}\")\n",
    "    print(f\"Tickers: {', '.join(framework_df['Ticker'].unique())}\")\n",
    "    print(f\"Date range: {framework_df['Date'].min()} to {framework_df['Date'].max()}\")\n",
    "    print(f\"\\n✅ Saved to: {output_file}\")\n",
    "    \n",
    "    # Show column list\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"COLUMNS IN ORDER\")\n",
    "    print(\"=\" * 60)\n",
    "    for i, col in enumerate(framework_df.columns, 1):\n",
    "        print(f\"{i:2d}. {col}\")\n",
    "    \n",
    "    # Preview\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"PREVIEW (First 5 records)\")\n",
    "    print(\"=\" * 60)\n",
    "    preview_cols = ['Date', 'Ticker', 'Close', 'Return_Pct', 'EPS', \n",
    "                    'Revenue', 'Fed_Funds_Rate', 'GoogleTrends']\n",
    "    available_preview = [c for c in preview_cols if c in framework_df.columns]\n",
    "    print(framework_df[available_preview].head(5).to_string(index=False))\n",
    "    \n",
    "    # Data quality\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"DATA QUALITY\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"\\nMissing values:\")\n",
    "    missing = framework_df.isnull().sum()\n",
    "    if missing.sum() > 0:\n",
    "        print(missing[missing > 0].to_string())\n",
    "    else:\n",
    "        print(\"✅ No missing values!\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"SAMPLE ROW (Like Framework)\")\n",
    "    print(\"=\" * 60)\n",
    "    # Show one row formatted nicely\n",
    "    sample = framework_df.iloc[0]\n",
    "    for col, val in sample.items():\n",
    "        if pd.notna(val):\n",
    "            if isinstance(val, float):\n",
    "                print(f\"{col:20s}: {val:.2f}\")\n",
    "            else:\n",
    "                print(f\"{col:20s}: {val}\")\n",
    "    \n",
    "    return framework_df\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    df = create_framework_dataset()\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"✅ FRAMEWORK DATASET READY!\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"\\nThis matches your original framework structure.\")\n",
    "    print(\"Use this for:\")\n",
    "    print(\"  → Excel analysis\")\n",
    "    print(\"  → Power BI dashboards\")\n",
    "    print(\"  → Quick exploration\")\n",
    "    print(\"\\nFile: data/features/analysis_dataset.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b71c267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STARTING COMPLETE DATA COLLECTION PIPELINE\n",
      "\n",
      "============================================================\n",
      "STEP 1: COLLECTING YAHOO FINANCE PRICE DATA\n",
      "============================================================\n",
      "\n",
      "Tickers: AAPL, GOOGL, MSFT\n",
      "Period: 2010-10-13 to 2025-10-09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yashs\\AppData\\Local\\Temp\\ipykernel_6972\\2846630113.py:49: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
      "  wide_df = yf.download(ALL_TICKERS, start=start_date, end=end_date)\n",
      "[*********************100%***********************]  3 of 3 completed\n",
      "C:\\Users\\yashs\\AppData\\Local\\Temp\\ipykernel_6972\\2846630113.py:58: FutureWarning: The previous implementation of stack is deprecated and will be removed in a future version of pandas. See the What's New notes for pandas 2.1.0 for details. Specify future_stack=True to adopt the new implementation and silence this warning.\n",
      "  long_df = wide_df.stack(level=1).reset_index()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Price data for 3 stocks saved correctly to: data/raw/stock_prices_raw.csv\n",
      "\n",
      "============================================================\n",
      "STEP 2: CALCULATING STOCK METRICS\n",
      "============================================================\n",
      "\n",
      "✅ Data with technical metrics saved to: data/processed/stock_prices_with_metrics.csv\n",
      "\n",
      "============================================================\n",
      "✅ DATA COLLECTION STEPS 1 & 2 COMPLETE!\n",
      "============================================================\n",
      "\n",
      "The raw price data has been fixed. You can now run the subsequent steps.\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# --- COMPLETE DATA COLLECTION PIPELINE (Corrected) ---\n",
    "# This single script runs all data collection and processing steps.\n",
    "# The issue with incorrect CSV formatting in Step 1 has been fixed.\n",
    "# ==============================================================================\n",
    "\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from fredapi import Fred\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import os\n",
    "\n",
    "# ==============================================================================\n",
    "# --- CONFIGURATION ---\n",
    "# ==============================================================================\n",
    "# --- API KEYS ---\n",
    "ALPHA_VANTAGE_API_KEY = '6RTZP2WX16TXZ8G3'\n",
    "FRED_API_KEY = '6a685736008918f95526f561e6b05b77'\n",
    "\n",
    "# --- TICKERS ---\n",
    "# Define your single source of truth for tickers here\n",
    "ALL_TICKERS = ['AAPL', 'GOOGL', 'MSFT'] \n",
    "\n",
    "# --- DATE RANGE ---\n",
    "YEARS_OF_DATA = 15\n",
    "\n",
    "# ==============================================================================\n",
    "# --- STEP 1: YAHOO FINANCE PRICE DATA (Corrected Method) ---\n",
    "# ==============================================================================\n",
    "def collect_price_data():\n",
    "    \"\"\"\n",
    "    Downloads stock price data and saves it in the correct \"long\" format.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"STEP 1: COLLECTING YAHOO FINANCE PRICE DATA\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    end_date = datetime.now()\n",
    "    start_date = end_date - timedelta(days=YEARS_OF_DATA * 365)\n",
    "    \n",
    "    print(f\"\\nTickers: {', '.join(ALL_TICKERS)}\")\n",
    "    print(f\"Period: {start_date.strftime('%Y-%m-%d')} to {end_date.strftime('%Y-%m-%d')}\")\n",
    "    \n",
    "    try:\n",
    "        # Download all tickers at once for efficiency. This creates a multi-level column index.\n",
    "        wide_df = yf.download(ALL_TICKERS, start=start_date, end=end_date)\n",
    "        \n",
    "        if wide_df.empty:\n",
    "            print(\"\\n❌ No price data collected! Check tickers or internet connection.\")\n",
    "            return\n",
    "\n",
    "        # --- CRITICAL FIX: Reshape the data from \"wide\" to \"long\" format ---\n",
    "        # stack(level=1) pivots the ticker names from columns into a new index level.\n",
    "        # reset_index() converts the Date and new Ticker index levels into columns.\n",
    "        long_df = wide_df.stack(level=1).reset_index()\n",
    "        long_df.rename(columns={'level_1': 'Ticker'}, inplace=True)\n",
    "        \n",
    "        # Reorder and select the columns we need\n",
    "        final_df = long_df[['Date', 'Ticker', 'Open', 'High', 'Low', 'Close', 'Volume']]\n",
    "        \n",
    "        os.makedirs('data/raw', exist_ok=True)\n",
    "        output_file = 'data/raw/stock_prices_raw.csv'\n",
    "        final_df.to_csv(output_file, index=False)\n",
    "        \n",
    "        print(f\"\\n✅ Price data for {len(ALL_TICKERS)} stocks saved correctly to: {output_file}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ An error occurred during download or processing: {e}\")\n",
    "\n",
    "# ==============================================================================\n",
    "# --- STEP 2: CALCULATE TECHNICAL METRICS ---\n",
    "# ==============================================================================\n",
    "def add_metrics():\n",
    "    \"\"\"\n",
    "    Loads raw price data and adds calculated metrics.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"STEP 2: CALCULATING STOCK METRICS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv('data/raw/stock_prices_raw.csv')\n",
    "        df['Date'] = pd.to_datetime(df['Date'])\n",
    "    except FileNotFoundError:\n",
    "        print(\"\\n❌ Raw price data not found. Please run Step 1 first.\")\n",
    "        return\n",
    "\n",
    "    # Convert columns to numeric, coercing errors. This prevents crashes.\n",
    "    numeric_cols = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
    "    for col in numeric_cols:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    df.dropna(subset=numeric_cols, inplace=True)\n",
    "        \n",
    "    df = df.sort_values(['Ticker', 'Date']).reset_index(drop=True)\n",
    "    \n",
    "    df['Return_Pct'] = df.groupby('Ticker')['Close'].pct_change() * 100\n",
    "    df['Price_Range'] = df['High'] - df['Low']\n",
    "    df['MA_50'] = df.groupby('Ticker')['Close'].transform(lambda x: x.rolling(50, min_periods=25).mean())\n",
    "    df['MA_200'] = df.groupby('Ticker')['Close'].transform(lambda x: x.rolling(200, min_periods=100).mean())\n",
    "    \n",
    "    os.makedirs('data/processed', exist_ok=True)\n",
    "    output_file = 'data/processed/stock_prices_with_metrics.csv'\n",
    "    df.to_csv(output_file, index=False)\n",
    "    \n",
    "    print(f\"\\n✅ Data with technical metrics saved to: {output_file}\")\n",
    "\n",
    "# ==============================================================================\n",
    "# --- STEP 3 & 4: FUNDAMENTALS & MACRO DATA ---\n",
    "# These functions should work correctly with the properly formatted inputs.\n",
    "# No changes are needed here.\n",
    "# ==============================================================================\n",
    "\n",
    "# ... (Functions for Steps 3, 4, 5, and 6 would go here, unchanged) ...\n",
    "# For brevity, I will omit the full code for the other steps as they are correct,\n",
    "# but in your final file, you would paste them here. This example focuses on the fix.\n",
    "\n",
    "# ==============================================================================\n",
    "# --- MAIN EXECUTION ---\n",
    "# ==============================================================================\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"STARTING COMPLETE DATA COLLECTION PIPELINE\\n\")\n",
    "    \n",
    "    # Run Step 1: Get correctly formatted price data\n",
    "    collect_price_data()\n",
    "    \n",
    "    # Run Step 2: Calculate metrics\n",
    "    add_metrics()\n",
    "    \n",
    "    # In a full script, you would continue to call the functions for:\n",
    "    # collect_fundamentals()\n",
    "    # get_macro_data()\n",
    "    # collect_news_sentiment_data()\n",
    "    # forward_fill_fundamentals() & backfill_sentiment_data()\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"✅ DATA COLLECTION STEPS 1 & 2 COMPLETE!\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"\\nThe raw price data has been fixed. You can now run the subsequent steps.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f518c5c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "RUNNING SANITY CHECK ON FINAL MASTER DATASET\n",
      "============================================================\n",
      "\n",
      "1. Checking for file: data/features/Stocks_dataset.csv\n",
      "   ✅ File found and loaded successfully.\n",
      "\n",
      "2. Checking basic structure...\n",
      "   - Shape: 11013 rows, 17 columns\n",
      "   - Tickers found: AAPL, GOOGL, MSFT\n",
      "\n",
      "3. Checking date range...\n",
      "   - Date range: 2011-03-08 to 2025-10-09\n",
      "\n",
      "4. Checking data integrity...\n",
      "   ✅ No negative prices or volumes found.\n",
      "   ✅ No duplicate rows found.\n",
      "\n",
      "5. Analyzing missing values...\n",
      "   ✅ No missing values found anywhere in the dataset!\n",
      "\n",
      "============================================================\n",
      "FINAL VERDICT\n",
      "============================================================\n",
      "   ✅ SANITY CHECK PASSED.\n",
      "   The dataset is clean, complete, and ready for analysis or machine learning.\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# --- Sanity Check for the Final Master Dataset ---\n",
    "# This script performs a series of diagnostic checks on the final, clean\n",
    "# master dataset to identify any remaining issues.\n",
    "# It does NOT modify any files; it only reads and reports.\n",
    "# ==============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def run_sanity_check():\n",
    "    \"\"\"\n",
    "    Runs a full diagnostic check on the clean master dataset.\n",
    "    \"\"\"\n",
    "    # This points to the final output of your main pipeline script.\n",
    "    master_file = 'data/features/Stocks_dataset.csv'\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"RUNNING SANITY CHECK ON FINAL MASTER DATASET\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # --- 1. File Existence and Loading ---\n",
    "    print(f\"\\n1. Checking for file: {master_file}\")\n",
    "    if not os.path.exists(master_file):\n",
    "        print(\"   ❌ FAIL: Final master dataset file not found.\")\n",
    "        print(\"   Please ensure the full_pipeline.py script has been run successfully.\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(master_file, low_memory=False)\n",
    "        df['Date'] = pd.to_datetime(df['Date'])\n",
    "        print(\"   ✅ File found and loaded successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ FAIL: Could not load the CSV file. Error: {e}\")\n",
    "        return\n",
    "\n",
    "    # --- Initialize Verdict ---\n",
    "    issues_found = []\n",
    "\n",
    "    # --- 2. Basic Structure Check ---\n",
    "    print(\"\\n2. Checking basic structure...\")\n",
    "    print(f\"   - Shape: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "    expected_tickers = ['AAPL', 'GOOGL', 'MSFT']\n",
    "    found_tickers = sorted(df['Ticker'].unique())\n",
    "    print(f\"   - Tickers found: {', '.join(found_tickers)}\")\n",
    "    if set(expected_tickers) != set(found_tickers):\n",
    "        issues_found.append(f\"Ticker mismatch! Expected {expected_tickers} but found {found_tickers}.\")\n",
    "\n",
    "    # --- 3. Date Range and Continuity ---\n",
    "    print(\"\\n3. Checking date range...\")\n",
    "    min_date = df['Date'].min()\n",
    "    max_date = df['Date'].max()\n",
    "    print(f\"   - Date range: {min_date.strftime('%Y-%m-%d')} to {max_date.strftime('%Y-%m-%d')}\")\n",
    "    if (max_date - min_date).days < (14 * 365): # Check for roughly 15 years\n",
    "         issues_found.append(\"Date range is shorter than the expected ~15 years.\")\n",
    "\n",
    "    # --- 4. Data Integrity Check ---\n",
    "    print(\"\\n4. Checking data integrity...\")\n",
    "    # Check for negative prices or volume\n",
    "    if (df['Close'] <= 0).any() or (df['Volume'] < 0).any():\n",
    "        issues_found.append(\"Invalid data found: Close price is zero/negative or Volume is negative.\")\n",
    "        print(\"   ❌ FAIL: Negative or zero values found in 'Close' or 'Volume'.\")\n",
    "    else:\n",
    "        print(\"   ✅ No negative prices or volumes found.\")\n",
    "\n",
    "    # Check for duplicate rows\n",
    "    if df.duplicated().any():\n",
    "        issues_found.append(\"Duplicate rows found in the dataset.\")\n",
    "        print(\"   ❌ FAIL: Duplicate rows detected.\")\n",
    "    else:\n",
    "        print(\"   ✅ No duplicate rows found.\")\n",
    "        \n",
    "    # --- 5. Missing Value Analysis ---\n",
    "    print(\"\\n5. Analyzing missing values...\")\n",
    "    total_missing = df.isnull().sum().sum()\n",
    "    \n",
    "    if total_missing > 0:\n",
    "        missing_pct = (df.isnull().sum() / len(df) * 100).sort_values(ascending=False)\n",
    "        print(\"   - Percentage of missing values per column:\")\n",
    "        print(missing_pct[missing_pct > 0].round(2).to_string())\n",
    "        issues_found.append(f\"Found {total_missing} total missing values in the dataset.\")\n",
    "    else:\n",
    "        print(\"   ✅ No missing values found anywhere in the dataset!\")\n",
    "\n",
    "    # --- 6. Final Verdict ---\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"FINAL VERDICT\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    if issues_found:\n",
    "        print(\"   ❌ SANITY CHECK FAILED.\")\n",
    "        print(\"   The following issues were detected:\\n\")\n",
    "        for i, issue in enumerate(issues_found, 1):\n",
    "            print(f\"   {i}. {issue}\")\n",
    "        print(\"\\n   RECOMMENDATION: Review the pipeline steps. The cleaning process should handle these issues.\")\n",
    "    else:\n",
    "        print(\"   ✅ SANITY CHECK PASSED.\")\n",
    "        print(\"   The dataset is clean, complete, and ready for analysis or machine learning.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_sanity_check()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264440c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
